# Tensor2Tensor and Google Colab
This project is aim to create a model that can insert punctuations to text in Vietnamese by using mainly [tensor2tensor](https://github.com/tensorflow/tensor2tensor) and [Google Colab](https://research.google.com/colaboratory/faq.html).
## Quick start with built-in problems and models
T2T has pre-defined many problems and models for you and provide very nice datasets for you to train your own model.
### Set up the environment in Google Colab
```
!pip install -q -U tensor2tensor 
!pip install -q tensorflow matplotlib
```
And remember to change the Runtime type to GPU as it is fastest in our experience.
### Mount Google Drive
By Mouting with Google Drive, all folders, data that you generate will be kept in Drive. If not, they will be wiped out after the virtual machine reset the runtime.dd
```
from google.colab import drive
drive.mount('/content/gdrive')
```
There appears a link to get authentication code. After authentication, you will see a folder named gdrive in content/. Then you can use this virtual machine to access files and data inside your Drive.
### Choosing problem
You can choose a problems from t2t to train. To see the list of problems, let run:
```
from tensor2tensor import problems
problems.available()
```
Let say I want to train a translator from English to Vietnamese. then I choose __translate_envi_iwslt32k__
Use __transformer__ model to train with the hparams_set is __transformer_base__
### Generate data
```
!t2t-datagen \
  --data_dir=gdrive/My\ Drive/Colab/data \
  --tmp_dir=gdrive/My\ Drive/Colab/tmp \
  --problem=translate_envi_iwslt32k
  ```
T2T will go to the link for dataset (specified in the  [translate_envi_iwslt32k.py](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/translate_envi.py)) download the raw data to tmp, extract it then then built up the "real data files" in data folder (I think so, but I cannot read those filesd) and by default there are 100 "real data file" plus another file name vocab. This file store all the vocabulary of the dataset we provide, one line one word.
### Trainning
```
!t2t-trainer \
  --problem="translate_envi_customized" \
  --output_dir=gdrive/My\ Drive/Colab/train \
  --model=transformer \
  --hparams_set=transformer_base \
  --train_steps=100000 \
  --eval_steps=1000 \
  --data_dir=gdrive/My\ Drive/Colab/data
  ```
In our points of view, 100000 steps train is far enough and the translator is already nice. But training took times, too much time so actually we don't train 100000 steps for the first time but only 10000. Since all data is saved in Google Drive, after training first 10000 steps, we can disconnect to the virtual machine and do something more funny and then bach to connect again, mount our Drive and re-execute this training snipet but with **--train_steps=20000**. The machine will automatically train for us from step 10000 to 20000. And so on,...
### Running Decode
There are two way you can run the decode. First is decode from file to file:
```
!t2t-decoder \
  --data_dir=gdrive/My\ Drive/Colab/data \
  --problem="translate_envi_customized" \
  --model=transformer \
  --hparams_set=transformer_base \
  --output_dir=gdrive/My\ Drive/Colab/train \
  --decode_hparams="beam_size=6,alpha=0.6" \
  --decode_from_file=gdrive/My\ Drive/Colab/in.txt \
  --decode_to_file=gdrive/My\ Drive/Colab/out.txt
  ```
To run this, you must push the English sentence into in.txt and upload it to the specified foler.
The other way is decode interactive that will open a box for you to enter the sentence on runtime. Just only remove the __--decode_from_file__ and __decode_to_file__ flags and replace by __--decode_interactive__
Original instruction [here](https://github.com/tensorflow/tensor2tensor/blob/master/README.md)
## Using Customized Dataset
Now we want to use another dataset for this translator. The only thing to do is creating a new folder called usr and add a flag __--t2t_usr_di=usr_dir__ for **t2t-datagen**, **t2t-trainer** and **t2t-decoder** command. Inside usr dir, there are two file, the main one is [__mycustomized.py__](https://github.com/linhhonblade/try-tensor2tensor/blob/master/custom_data/usr_example/my_customized.py) which register a new problem name __translate_envi_customized__ with our own dataset. Source code for training with customized data is in [this drive](https://drive.google.com/drive/folders/1NBh7bwgI1vAhaUKJuDisXAqDOhrgHg1J?usp=sharing)
For more instruction, please see [this](https://github.com/tensorflow/tensor2tensor/blob/master/docs/new_problem.md).

## Deployment
Refer to this [repo](https://github.com/larycoder/chatbots) for deployment
